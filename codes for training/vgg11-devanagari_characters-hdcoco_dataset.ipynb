{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10876462,"sourceType":"datasetVersion","datasetId":6757863}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:40:50.698716Z","iopub.execute_input":"2025-02-28T04:40:50.698919Z","iopub.status.idle":"2025-02-28T04:41:47.451167Z","shell.execute_reply.started":"2025-02-28T04:40:50.698899Z","shell.execute_reply":"2025-02-28T04:41:47.450106Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import models, transforms, datasets\nfrom tqdm import tqdm\nfrom PIL import Image\nimport cv2\n\n# Hyperparameters from the figure\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\nMOMENTUM = 0.9\nSTEP_SIZE = 7\nGAMMA = 0.1\nEPOCHS = 15","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:50:40.802177Z","iopub.execute_input":"2025-02-28T04:50:40.802526Z","iopub.status.idle":"2025-02-28T04:50:46.293697Z","shell.execute_reply.started":"2025-02-28T04:50:40.802498Z","shell.execute_reply":"2025-02-28T04:50:46.292792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")  # Use the first CUDA device (P100 on Kaggle)\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")  # Should show Tesla P100\n    # Set some CUDA options for better performance\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, using CPU instead\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:50:50.949361Z","iopub.execute_input":"2025-02-28T04:50:50.949897Z","iopub.status.idle":"2025-02-28T04:50:51.042637Z","shell.execute_reply.started":"2025-02-28T04:50:50.949864Z","shell.execute_reply":"2025-02-28T04:50:51.041832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data transformations\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),  # VGG11 requires 224x224 input\n        transforms.RandomHorizontalFlip(p=0.3),\n        transforms.RandomRotation(15),\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndef load_and_split_data(data_dir, train_pct=0.8):\n    \"\"\"Load the dataset and split into train and validation sets.\"\"\"\n    # Load the full dataset\n    full_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n    \n    # Calculate split sizes\n    train_size = int(len(full_dataset) * train_pct)\n    val_size = len(full_dataset) - train_size\n    \n    # Perform random split\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    \n    # Apply correct transformations to each split\n    train_dataset.dataset.transform = data_transforms['train']\n    val_dataset.dataset.transform = data_transforms['val']\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    \n    class_names = full_dataset.classes\n    num_classes = len(class_names)\n    \n    return train_loader, val_loader, class_names, num_classes\n\ndef initialize_model(num_classes):\n    \"\"\"Initialize the VGG11 model with pretrained weights and modify the classifier.\"\"\"\n    # Load pretrained VGG11 model\n    model = models.vgg11(pretrained=True)\n    \n    # Freeze feature layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n    \n    # Modify the classifier for Devanagari characters\n    model.classifier = nn.Sequential(\n        nn.Linear(512 * 7 * 7, 4096),\n        nn.ReLU(True),\n        nn.Dropout(),\n        nn.Linear(4096, 4096),\n        nn.ReLU(True),\n        nn.Dropout(),\n        nn.Linear(4096, num_classes)\n    )\n    \n    return model\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=15):\n    \"\"\"Train the model and track metrics.\"\"\"\n    model.to(device)\n    \n    # Initialize tracking variables\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_acc': [],\n        'val_acc': []\n    }\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        # Iterate over data\n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        # Step the scheduler\n        scheduler.step()\n        \n        # Calculate epoch training metrics\n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n        \n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc.item())\n        \n        print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n        \n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        # No gradient tracking needed\n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Forward pass\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n        \n        # Calculate epoch validation metrics\n        epoch_val_loss = running_loss / len(val_loader.dataset)\n        epoch_val_acc = running_corrects.double() / len(val_loader.dataset)\n        \n        history['val_loss'].append(epoch_val_loss)\n        history['val_acc'].append(epoch_val_acc.item())\n        \n        print(f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n    \n    return model, history\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:52:16.714971Z","iopub.execute_input":"2025-02-28T04:52:16.715277Z","iopub.status.idle":"2025-02-28T04:52:16.728699Z","shell.execute_reply.started":"2025-02-28T04:52:16.715253Z","shell.execute_reply":"2025-02-28T04:52:16.727816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics(history):\n    \"\"\"Plot training and validation metrics.\"\"\"\n    # Set up the figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot loss\n    ax1.plot(history['train_loss'], label='Training Loss')\n    ax1.plot(history['val_loss'], label='Validation Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Validation Loss')\n    ax1.legend()\n    \n    # Plot accuracy\n    ax2.plot(history['train_acc'], label='Training Accuracy')\n    ax2.plot(history['val_acc'], label='Validation Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Training and Validation Accuracy')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_metrics.png')\n    plt.show()\n\ndef create_confusion_matrix(model, val_loader, class_names):\n    \"\"\"Create and plot the confusion matrix.\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader):\n            inputs = inputs.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            # Collect predictions and true labels\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(15, 15))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png')\n    plt.show()\n    \n    return cm\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:52:29.691984Z","iopub.execute_input":"2025-02-28T04:52:29.692307Z","iopub.status.idle":"2025-02-28T04:52:29.699648Z","shell.execute_reply.started":"2025-02-28T04:52:29.692280Z","shell.execute_reply":"2025-02-28T04:52:29.698805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(data_dir):\n    \"\"\"Main function to run the training and evaluation pipeline.\"\"\"\n    # Load and split the data\n    train_loader, val_loader, class_names, num_classes = load_and_split_data(data_dir, train_pct=0.8)\n    print(f\"Number of classes: {num_classes}\")\n    print(f\"Classes: {class_names}\")\n    \n    # Initialize the model\n    model = initialize_model(num_classes)\n    \n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n    \n    # Define learning rate scheduler\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n    \n    # Train the model\n    trained_model, history = train_model(\n        model, \n        train_loader, \n        val_loader, \n        criterion, \n        optimizer, \n        scheduler, \n        num_epochs=EPOCHS\n    )\n    \n    # Plot the training metrics\n    plot_metrics(history)\n    \n    # Create confusion matrix\n    cm = create_confusion_matrix(trained_model, val_loader, class_names)\n    \n    # Save the model\n    torch.save(trained_model.state_dict(), 'devanagari_vgg11_model.pth')\n    print(\"Model saved to 'devanagari_vgg11_model.pth'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:52:34.310879Z","iopub.execute_input":"2025-02-28T04:52:34.311183Z","iopub.status.idle":"2025-02-28T04:52:34.316698Z","shell.execute_reply.started":"2025-02-28T04:52:34.311160Z","shell.execute_reply":"2025-02-28T04:52:34.315929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Replace with your dataset path\n    data_dir = \"/kaggle/input/dataset-major\"\n    main(data_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:52:59.789037Z","iopub.execute_input":"2025-02-28T04:52:59.789321Z","iopub.status.idle":"2025-02-28T05:19:37.619193Z","shell.execute_reply.started":"2025-02-28T04:52:59.789300Z","shell.execute_reply":"2025-02-28T05:19:37.618148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}